# Pseudo-civility-comments-detection  
*A Weak Supervision + Gold Refinement Approach using BERT*

---

## ğŸ“˜ Project Overview

Pseudo-civility refers to comments that **appear polite on the surface** (e.g., containing courteous expressions) but **carry negative pragmatic intent**.  
This project builds an **English pseudo-civility classifier** using:

- Multi-source weak labels  
- Rule-based pseudo-civil candidate mining  
- A small manually annotated gold dataset  
- Two-stage BERT fine-tuning  
- Evaluation analysis  

---

## ğŸ“š Task Definition

**Pseudo-civil comments** are defined as:

> â€œComments with polite linguistic surface forms (e.g., â€˜thank youâ€™, â€˜with all due respectâ€™)  
>  **but carrying negative pragmatic intent**, such as belittling intelligence, dismissing ideas,  
>  covert hostility, refined sarcasm, etc.â€

We classify comments into **three categories**:

| Label | Meaning |
|-------|---------|
| `civil` | Genuinely polite or neutral comments, even when expressing disagreement |
| `uncivil` | Explicit insults, profanity, toxic hostility |
| `pseudo_civil` | Polite surface + negative semantic/pragmatic intent |

---

## ğŸ“¦ Data Sources

We integrated **four public datasets** to build weak supervision signals:

### 1. **Wikipedia Toxicity (Jigsaw)**
- Provides explicit toxicity attributes.
- Used as the main source for *uncivil* weak labels.

### 2. **Wikipedia Personal Attacks**
- Contains crowdsourced scores for attack severity.
- Used to enrich *uncivil* and *pseudo-civil* candidates.

### 3. **Stanford Politeness Corpus**
- Used to detect polite markers and polite surface cues.

### 4. **Friends TV Dialogue Corpus**
- A large set of natural, mostly non-toxic dialog.
- Used as the source of *civil* weak labels.

---

## ğŸ—ï¸ Weak Supervision Pipeline

We built a weakly-labeled dataset using heuristic rules:

### **Polite markers**  
Extracted via regex: thank you, thanks, please, I appreciate, with all due respect, kindly, sorry, no offense


### **Toxicity scores (0â€“1)**
Sourced from Wikipedia datasets.

### **Weak labeling rules**
| Condition | Label |
|----------|--------|
| `tox_score` â‰¤ 0.1 & no profanity | **civil** |
| `tox_score` â‰¥ 0.7 & profanity words | **uncivil** |
| polite markers present & 0.3 â‰¤ `tox_score` â‰¤ 0.8 | **pseudo_civil (weak)** |

Generated file:  
weak_labeled_dataset.csv


---

## ğŸ” Pseudo-Civil Candidate Filtering

To refine pseudo-civil candidates, we applied:

- **Exclude heavy profanity**
- **Keep polite markers**
- **Keep mild negative semantic cues** such as:  
  `nonsense, ridiculous, ignorant, garbage, pathetic, no sense, useless`
- **Keep mid-range toxicity**

Generated file:
pseudo_civil_filtered_for_manual.csv


---

## âœï¸ Manual Annotation (Gold Data)

From the filtered set, **~200 comments** were manually labeled into:

- `civil`
- `uncivil`
- `pseudo_civil`

Final gold file:
gold_small.csv

To balance the severely underrepresented `uncivil` class,  
we augmented the gold set with **80 high-confidence uncivil sentences** from the weak dataset:
gold_augmented.csv


---

## ğŸ“ Annotation Guidelines (Final Version)

### **1. Civil**
A comment is labeled **civil** if:
- It conveys disagreement politely  
- It avoids attacking the interlocutor  
- It uses reasoning rather than belittlement  

**Examples**  
- â€œI understand your point, but I disagree.â€  
- â€œThank you for your explanation; I see it differently.â€

---

### **2. Uncivil**
A comment is **uncivil** if:
- Contains profanity / insults  
- Targets the interlocutorâ€™s identity, intelligence, or intent  
- Exhibits explicit hostility  

**Examples**  
- â€œYou are an idiot.â€  
- â€œThis is the dumbest thing Iâ€™ve ever read.â€

---

### **3. Pseudo-Civil**
A comment is **pseudo-civil** if:
- Contains polite surface cues  
- AND implicitly attacks competence / intelligence  
- AND contains dismissive or belittling implications  

**Examples**
- â€œThank you for your brilliant idea, though it makes absolutely no sense.â€  
- â€œWith all due respect, no reasonable person would believe that.â€

This category requires pragmatic judgment.

---

## ğŸ¤– Model Architecture

We use **BERT-base-uncased** as a contextual embedding model.

### **Two-stage training pipeline**

#### **Stage 1 â€” Weak Supervision Training**
- Train BERT on `train_weak_balanced.csv`
- Learns coarse-grained distinctions (civil vs toxic)

#### **Stage 2 â€” Gold Refinement**
- Load `bert_weak_model`  
- Fine-tune on `gold_augmented.csv`  
- Use small learning rate `1e-5`  
- 5 epochs  

Purpose:
> Correct biases created by weak labels  
> and teach the model subtle pragmatic phenomena.

---

## ğŸ“Š Experimental Results
### **1. Analyze Gold model**
=== æœ€ç»ˆè¯„ä¼°ç»“æœï¼ˆgold_small evalï¼‰ ===
- eval_loss: 0.6456559300422668
- eval_macro_f1: 0.8291776838741032
- eval_civil_f1: 0.851063829787234
- eval_uncivil_f1: 0.9047619047619048
- eval_pseudo_civil_f1: 0.7817073170731707
- eval_accuracy: 0.8307692307692308
- eval_runtime: 4.3529
- eval_samples_per_second: 14.933
- eval_steps_per_second: 1.149
- epoch: 5.0

### **2. Weak model vs Gold model performance (on the same gold evaluation set)**

| Model | Macro F1 | Civil F1 | Uncivil F1 | Pseudo-Civil F1 | Accuracy |
|-------|----------|----------|-------------|------------------|----------|
| Weak Model | **0.51** | **0.00** | 0.88 | 0.67 | 0.625 |
| Gold Model | **0.83** | 0.80 | 0.90 | 0.78 | **0.821** |

### ğŸ” Interpretation
- Weak model **fails completely** on civil (F1 = 0)  
  â†’ It treats almost all polite comments as pseudo-civil or uncivil  
- Gold model greatly improves civil detection (0 â†’ 0.80)  
- Pseudo-civil F1 increases from **0.67 â†’ 0.78**  
- Macro-F1 jumps from **0.51 â†’ 0.83**

## ğŸ§­ Future Work

### 1. **Larger annotated pseudo-civil dataset**
- Multi-round annotation  
- Clearer guidelines  
- Better edge-case handling

### 2. **Semi-supervised learning**
- Self-training with model confidence  
- Noise-robust loss functions (e.g., symmetric CE)

### 3. **Sarcasm & irony modeling**
- Integrate sarcasm detection models  
- Use discourse act classification  
- Capture pragmatic conflicts explicitly

### 4. **Multilingual / cross-lingual transfer**
- Apply XLM-R / mBERT  
- Zero-shot pseudo-civil transfer to Chinese  
- Collect polite-but-hostile multilingual corpora

### 5. **Evaluation of robustness & calibration**
- Temperature scaling  
- Expected Calibration Error (ECE)  
- Adversarial rewrite test (manual paraphrases)

  ---

## ğŸ“ Repository Structure (Recommended)
```bash
.
project/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ weak_labeled_dataset.csv
â”‚ â”œâ”€â”€ gold_small.csv
â”‚ â”œâ”€â”€ gold_augmented.csv
â”‚ â”œâ”€â”€ pseudo_civil_candidates.csv
â”‚ â”œâ”€â”€ pseudo_civil_filtered_for_manual.csv
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ bert_weak_model/
â”‚ â”œâ”€â”€ bert_gold_model/
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ prepare_datasets.py
â”‚ â”œâ”€â”€ build_candidates.py
â”‚ â”œâ”€â”€ filter_pseudo_civil.py
â”‚ â”œâ”€â”€ make_gold_augmented.py
â”‚ â”œâ”€â”€ train_bert_weak.py
â”‚ â”œâ”€â”€ train_bert_gold.py
â”‚ â”œâ”€â”€ compare_weak_vs_gold.py
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
